{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1sODlP3Y9CIYShztpGO3Aq2OEyBp8ahhz",
      "authorship_tag": "ABX9TyPvA6J0OS9JfTW2Ewtx7fYk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guo-Weiqiang/Master-Project/blob/main/SEED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3azf08Yr0mhU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SeedDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data = np.load(file_path)\n",
        "        # self.labels = [i + 1 for i in [1,0,-1,-1,0,1,-1,0,1,1,0,-1,0,1,-1] ] * 3\n",
        "        self.labels = [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0] * 3 #\n",
        "        assert self.data.shape[1:] == (1, 62, 37001), \"Invalid shape of data\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the .npy file at the specified index\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # Convert the data to a PyTorch tensor\n",
        "        sample = torch.from_numpy(sample).float()  # Ensure data is float\n",
        "\n",
        "        # Return the data and a dummy label or the actual label if you have it\n",
        "        label = self.labels[idx]\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return sample, label"
      ],
      "metadata": {
        "id": "AQkz__nn4guF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dmy8KzYH_fTu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_features, train_labels = next(iter(train_dataloader))\n",
        "# print(f\"Feature batch shape: {train_features.size()}\")\n",
        "# print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "metadata": {
        "id": "66ku3TnjE3fc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class EEGNet_ReLU(torch.nn.Module):\n",
        "#     def __init__(self, n_output):\n",
        "#         super(EEGNet_ReLU, self).__init__()\n",
        "#         self.firstConv = nn.Sequential(\n",
        "#             nn.Conv2d(1, 16, kernel_size=(1,51), stride=(1,1), padding=(0,25),bias=False),\n",
        "#             nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "#         )\n",
        "#         self.depthwiseConv = nn.Sequential(\n",
        "#             nn.Conv2d(16, 32, kernel_size=(2,1), stride=(1,1), groups=8,bias=False),\n",
        "#             nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "#             nn.ReLU(),\n",
        "#             nn.AvgPool2d(kernel_size=(1,4), stride=(1,4),padding=0),\n",
        "#             nn.Dropout(p=0.35)\n",
        "#         )\n",
        "#         self.separableConv = nn.Sequential(\n",
        "#             nn.Conv2d(32, 32, kernel_size=(1,15), stride=(1,1), padding=(0,7),bias=False),\n",
        "#             nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "#             nn.ReLU(),\n",
        "#             nn.AvgPool2d(kernel_size=(1,8), stride=(1,8),padding=0),\n",
        "#             nn.Dropout(p=0.35),\n",
        "#             nn.Flatten()\n",
        "#         )\n",
        "\n",
        "#         self.classify = nn.Sequential(\n",
        "#             nn.Linear(2256512, 256, bias=True),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(256, n_output, bias=True)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.firstConv(x)\n",
        "#         out = self.depthwiseConv(out)\n",
        "#         features = self.separableConv(out)\n",
        "#         # print('the shape of features before the classifier is ', features.shape)\n",
        "#         out = self.classify(features)\n",
        "#         return out, features\n",
        "class EEGNet_ReLU(torch.nn.Module):\n",
        "    def __init__(self, n_output):\n",
        "        super(EEGNet_ReLU, self).__init__()\n",
        "        # Adjusted kernel sizes and added more pooling\n",
        "        self.firstConv = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=(1, 51), stride=(1, 1), padding=(0, 25), bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=(1, 4))\n",
        "        )\n",
        "        # Since we have only one channel, the depthwise convolution is just a regular convolution here\n",
        "        # Adjust the number of output channels if necessary\n",
        "        self.depthwiseConv = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=(62, 1), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=(1, 8)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        # The separable convolution may not be needed, depending on the dimensionality after depthwiseConv\n",
        "        self.classify = nn.Sequential(\n",
        "            # The input features to the classifier need to be calculated based on the previous layer's output\n",
        "\n",
        "            nn.Linear(36992, 256, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, n_output, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.firstConv(x)\n",
        "        features = self.depthwiseConv(out)\n",
        "        # print(features.shape)\n",
        "        # out will have reduced dimensionality, you need to calculate 'in_features' based on this.\n",
        "        out = self.classify(features)\n",
        "        return out, features\n",
        "\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        print(\"1\", X.shape, y.shape)\n",
        "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred, features = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        correct = 0\n",
        "        loss += loss_fn(pred, y).item()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            correct /= size\n",
        "            print(f\"Train Error: \\n Accuracy: {(100*correct):>0.1f}%, loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            pred, features = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "def main():\n",
        "    model = EEGNet_ReLU(3)\n",
        "    model.to(DEVICE)\n",
        "    summary(model,(1, 62, 37001))\n",
        "\n",
        "    train_data = SeedDataset('drive/MyDrive/EEGNet/processed_seed_dataset/subject1.npy')\n",
        "    test_data = SeedDataset('drive/MyDrive/EEGNet/processed_seed_dataset/subject2.npy')\n",
        "\n",
        "    train_dataloader = DataLoader(train_data, batch_size=5, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=5, shuffle=True)\n",
        "\n",
        "    # train_features, train_labels = next(iter(train_dataloader))\n",
        "    # print(f\"Feature batch shape: {train_features.size()}\")\n",
        "    # print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "\n",
        "    # The CrossEntropy loss function in PyTorch expects the target values to be in the range\n",
        "    # [0, C-1] where C is the number of classes.\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5, weight_decay=5e-4)\n",
        "\n",
        "    epochs = 100\n",
        "    for epoch in tqdm(range(1, epochs + 1)):\n",
        "        print(f\"Epoch {epoch}\\n-------------------------------\")\n",
        "        train(train_dataloader, model, loss_fn, optimizer)\n",
        "        test(test_dataloader, model, loss_fn)\n",
        "    print(\"Done!\")"
      ],
      "metadata": {
        "id": "D9ZAVyjVLdQt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main()"
      ],
      "metadata": {
        "id": "Btxg6AQNLyPz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data():\n",
        "    \"\"\"\n",
        "    two subjects: S4b, X11b\n",
        "    The experiment consists of 3 sessions for each subject. Each session consists of 4 to 9 runs\n",
        "    \"\"\"\n",
        "    train_data = np.load('drive/MyDrive/EEGNet/processed_seed_dataset/subject1.npy')\n",
        "    test_data = np.load('drive/MyDrive/EEGNet/processed_seed_dataset/subject2.npy')\n",
        "\n",
        "    train_label = [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0] * 3\n",
        "    test_label = [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0] * 3\n",
        "\n",
        "\n",
        "    mask = np.where(np.isnan(train_data))\n",
        "    train_data[mask] = np.nanmean(train_data)\n",
        "\n",
        "    mask = np.where(np.isnan(test_data))\n",
        "    test_data[mask] = np.nanmean(test_data)\n",
        "\n",
        "    train_data = torch.from_numpy(train_data).float()\n",
        "    test_data = torch.from_numpy(test_data).float()\n",
        "    train_label = torch.tensor(train_label, dtype=torch.long)\n",
        "    test_label = torch.tensor(test_label, dtype=torch.long)\n",
        "    val_data = test_data\n",
        "    val_label = test_label\n",
        "\n",
        "    print(train_data.shape, train_label.shape, val_data.shape, val_label.shape, test_data.shape, test_label.shape)\n",
        "\n",
        "    return train_data, train_label, val_data, val_label, test_data, test_label\n",
        "\n",
        "\n",
        "# source_data, source_label, val_data, val_label, target_data, target_label = read_data()"
      ],
      "metadata": {
        "id": "AHVqnjXwPxg9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(x_test,y_test,model,device):\n",
        "\n",
        "    # model.load_state_dict(torch.load(filepath))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        model.cuda(0)\n",
        "        n = x_test.shape[0]\n",
        "\n",
        "        # x_test = x_test.astype(\"float32\")\n",
        "        # y_test = y_test.astype(\"float32\").reshape(y_test.shape[0],)\n",
        "\n",
        "        x_test,y_test = x_test.to(device),y_test.to(device)\n",
        "        y_pred_test, features = model(x_test)\n",
        "\n",
        "        correct_test = (torch.max(y_pred_test,1)[1]==y_test).sum().item()\n",
        "        test_accuracy = correct_test / n\n",
        "        # print(\"testing accuracy:\",correct/n)\n",
        "\n",
        "    return test_accuracy, features\n",
        "\n",
        "\n",
        "def train(source_data, source_label, val_data, val_label, target_data, target_label, epochs=500, lr=1e-3):\n",
        "    max_training_accuracy = 0\n",
        "    max_test_accuracy = 0\n",
        "    device = torch.device(\"cuda:0\")\n",
        "\n",
        "    x, y = source_data, source_label\n",
        "\n",
        "    model = EEGNet_ReLU(n_output=3)\n",
        "    # print(model)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(),lr = lr)\n",
        "    # optimizer = optim.RMSprop(model.parameters(),lr = lr, momentum = 0.2)\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.5, weight_decay=5e-4)\n",
        "\n",
        "    model.cuda(0)\n",
        "    summary(model.cuda(),(1, 62, 37001))\n",
        "\n",
        "    loss_history = []\n",
        "    train_accuracy_history = []\n",
        "    val_accuracy_history = []\n",
        "    test_accuracy_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_pred, source_features = model(x)\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        sample_cnt = y.shape[0]\n",
        "        correct = (torch.max(y_pred,1)[1]==y).sum().item()\n",
        "        train_accuracy = correct / sample_cnt\n",
        "        train_accuracy_history.append(train_accuracy)\n",
        "\n",
        "        val_accuracy, val_features = test(val_data, val_label, model, device)\n",
        "        val_accuracy_history.append(val_accuracy)\n",
        "\n",
        "        # test_accuracy, target_features = test(target_data, target_label, model, device)\n",
        "        # test_accuracy_history.append(test_accuracy)\n",
        "\n",
        "        print(\"epochs:\",epoch,\"loss:\",loss.item(),\"D_s Accuracy:\",train_accuracy, \"D_t accuracy\", val_accuracy)\n",
        "\n",
        "        # max_training_accuracy = max(train_accuracy, max_training_accuracy)\n",
        "\n",
        "        # max_test_accuracy = max(test_accuracy, max_test_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "    # plt.figure(figsize=(8, 4))\n",
        "    # # plt.plot(loss_history, label=\"Loss\")\n",
        "    # plt.plot(train_accuracy_history, label='Train Accuracy')\n",
        "    # plt.plot(val_accuracy_history, label='Validation Accuracy')\n",
        "    # plt.plot(test_accuracy_history, label='Test Accuracy')\n",
        "    # plt.xlabel('Epochs')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.legend()\n",
        "    # plt.title('Accuracy Curve')\n",
        "    # plt.grid(True)\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "\n",
        "source_data, source_label, val_data, val_label, target_data, target_label = read_data()\n",
        "source_data = source_data[:20]\n",
        "source_label = source_label[:20]\n",
        "val_data = val_data[:20]\n",
        "val_label = val_label[:20]\n",
        "train(source_data, source_label, val_data, val_label, target_data, target_label, epochs=100, lr=1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4ZPSlPomPyUn",
        "outputId": "aa086871-3811-49aa-ace5-b358ed2cacfb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([45, 1, 62, 37001]) torch.Size([45]) torch.Size([45, 1, 62, 37001]) torch.Size([45]) torch.Size([45, 1, 62, 37001]) torch.Size([45])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 16, 62, 37001]             816\n",
            "       BatchNorm2d-2        [-1, 16, 62, 37001]              32\n",
            "              ReLU-3        [-1, 16, 62, 37001]               0\n",
            "         AvgPool2d-4         [-1, 16, 62, 9250]               0\n",
            "            Conv2d-5          [-1, 32, 1, 9250]          31,744\n",
            "       BatchNorm2d-6          [-1, 32, 1, 9250]              64\n",
            "              ReLU-7          [-1, 32, 1, 9250]               0\n",
            "         AvgPool2d-8          [-1, 32, 1, 1156]               0\n",
            "           Flatten-9                [-1, 36992]               0\n",
            "           Linear-10                  [-1, 256]       9,470,208\n",
            "             ReLU-11                  [-1, 256]               0\n",
            "           Linear-12                    [-1, 3]             771\n",
            "================================================================\n",
            "Total params: 9,503,635\n",
            "Trainable params: 9,503,635\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 8.75\n",
            "Forward/backward pass size (MB): 917.46\n",
            "Params size (MB): 36.25\n",
            "Estimated Total Size (MB): 962.47\n",
            "----------------------------------------------------------------\n",
            "epochs: 0 loss: 1.1324793100357056 D_s Accuracy: 0.3 D_t accuracy 0.35\n",
            "epochs: 1 loss: 105.3604965209961 D_s Accuracy: 0.35 D_t accuracy 0.35\n",
            "epochs: 2 loss: 116.85868072509766 D_s Accuracy: 0.45 D_t accuracy 0.3\n",
            "epochs: 3 loss: 83.26722717285156 D_s Accuracy: 0.5 D_t accuracy 0.3\n",
            "epochs: 4 loss: 73.26559448242188 D_s Accuracy: 0.45 D_t accuracy 0.3\n",
            "epochs: 5 loss: 12.311845779418945 D_s Accuracy: 0.55 D_t accuracy 0.3\n",
            "epochs: 6 loss: 0.7711173295974731 D_s Accuracy: 0.85 D_t accuracy 0.3\n",
            "epochs: 7 loss: 8.374608993530273 D_s Accuracy: 0.65 D_t accuracy 0.25\n",
            "epochs: 8 loss: 7.777979373931885 D_s Accuracy: 0.7 D_t accuracy 0.4\n",
            "epochs: 9 loss: 1.870208740234375 D_s Accuracy: 0.8 D_t accuracy 0.35\n",
            "epochs: 10 loss: 0.44708341360092163 D_s Accuracy: 0.9 D_t accuracy 0.4\n",
            "epochs: 11 loss: 0.012196311727166176 D_s Accuracy: 1.0 D_t accuracy 0.45\n",
            "epochs: 12 loss: 2.155280113220215 D_s Accuracy: 0.95 D_t accuracy 0.35\n",
            "epochs: 13 loss: 2.9200801849365234 D_s Accuracy: 0.95 D_t accuracy 0.35\n",
            "epochs: 14 loss: 2.470912218093872 D_s Accuracy: 0.9 D_t accuracy 0.35\n",
            "epochs: 15 loss: 0.9221129417419434 D_s Accuracy: 0.95 D_t accuracy 0.35\n",
            "epochs: 16 loss: 0.006741201970726252 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 17 loss: 0.003818484488874674 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 18 loss: 0.001263646176084876 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 19 loss: 0.0004445501253940165 D_s Accuracy: 1.0 D_t accuracy 0.3\n",
            "epochs: 20 loss: 0.0002078065590467304 D_s Accuracy: 1.0 D_t accuracy 0.3\n",
            "epochs: 21 loss: 0.00013719193520955741 D_s Accuracy: 1.0 D_t accuracy 0.3\n",
            "epochs: 22 loss: 0.00012731111200992018 D_s Accuracy: 1.0 D_t accuracy 0.2\n",
            "epochs: 23 loss: 0.06672317534685135 D_s Accuracy: 0.95 D_t accuracy 0.15\n",
            "epochs: 24 loss: 0.024547966197133064 D_s Accuracy: 1.0 D_t accuracy 0.2\n",
            "epochs: 25 loss: 0.00012469073408283293 D_s Accuracy: 1.0 D_t accuracy 0.3\n",
            "epochs: 26 loss: 0.0001104524708352983 D_s Accuracy: 1.0 D_t accuracy 0.3\n",
            "epochs: 27 loss: 0.00010388369992142543 D_s Accuracy: 1.0 D_t accuracy 0.3\n",
            "epochs: 28 loss: 0.00010413608833914623 D_s Accuracy: 1.0 D_t accuracy 0.3\n",
            "epochs: 29 loss: 0.0001108562937588431 D_s Accuracy: 1.0 D_t accuracy 0.3\n",
            "epochs: 30 loss: 0.00012379664985928684 D_s Accuracy: 1.0 D_t accuracy 0.3\n",
            "epochs: 31 loss: 0.00014198252756614238 D_s Accuracy: 1.0 D_t accuracy 0.3\n",
            "epochs: 32 loss: 0.0001631258928682655 D_s Accuracy: 1.0 D_t accuracy 0.3\n",
            "epochs: 33 loss: 0.00018357600492890924 D_s Accuracy: 1.0 D_t accuracy 0.3\n",
            "epochs: 34 loss: 0.0001987857249332592 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 35 loss: 0.00020471730385906994 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 36 loss: 0.0001995460334001109 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 37 loss: 0.0001842371711973101 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 38 loss: 0.00016205603606067598 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 39 loss: 0.00013699469855055213 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 40 loss: 0.00011245706264162436 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 41 loss: 9.059187141247094e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 42 loss: 7.232520147226751e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 43 loss: 5.7702465710463e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 44 loss: 4.631576666724868e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 45 loss: 3.760135587072e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 46 loss: 3.098267188761383e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 47 loss: 2.596001468191389e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 48 loss: 2.215844506281428e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 49 loss: 1.926239565364085e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 50 loss: 1.8559210730018094e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 51 loss: 1.8332757463213056e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 52 loss: 1.810630419640802e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 53 loss: 1.7903685147757642e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 54 loss: 1.7724905774230137e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 55 loss: 1.756400342856068e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 56 loss: 1.742098174872808e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 57 loss: 1.7283913621213287e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 58 loss: 1.7152808140963316e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n",
            "epochs: 59 loss: 1.7039581507560797e-05 D_s Accuracy: 1.0 D_t accuracy 0.35\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f96b0cc77e9c>\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mval_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-f96b0cc77e9c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(source_data, source_label, val_data, val_label, target_data, target_label, epochs, lr)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUSj4w8LNUBp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}