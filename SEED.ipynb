{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1sODlP3Y9CIYShztpGO3Aq2OEyBp8ahhz",
      "authorship_tag": "ABX9TyNIhH9XiNqv+v64b/jPwQtd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guo-Weiqiang/Master-Project/blob/main/SEED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3azf08Yr0mhU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SeedDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data = np.load(file_path)\n",
        "        self.labels = [i + 1 for i in [1,0,-1,-1,0,1,-1,0,1,1,0,-1,0,1,-1] ] * 3\n",
        "        assert self.data.shape[1:] == (1, 62, 37001), \"Invalid shape of data\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the .npy file at the specified index\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # Convert the data to a PyTorch tensor\n",
        "        sample = torch.from_numpy(sample).float()  # Ensure data is float\n",
        "\n",
        "        # Return the data and a dummy label or the actual label if you have it\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return sample, label"
      ],
      "metadata": {
        "id": "AQkz__nn4guF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dmy8KzYH_fTu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_features, train_labels = next(iter(train_dataloader))\n",
        "# print(f\"Feature batch shape: {train_features.size()}\")\n",
        "# print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "metadata": {
        "id": "66ku3TnjE3fc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGNet_ReLU(torch.nn.Module):\n",
        "    def __init__(self, n_output):\n",
        "        super(EEGNet_ReLU, self).__init__()\n",
        "        self.firstConv = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=(1,51), stride=(1,1), padding=(0,25),bias=False),\n",
        "            nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        self.depthwiseConv = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=(2,1), stride=(1,1), groups=8,bias=False),\n",
        "            nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=(1,4), stride=(1,4),padding=0),\n",
        "            nn.Dropout(p=0.35)\n",
        "        )\n",
        "        self.separableConv = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=(1,15), stride=(1,1), padding=(0,7),bias=False),\n",
        "            nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=(1,8), stride=(1,8),padding=0),\n",
        "            nn.Dropout(p=0.35),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        self.classify = nn.Sequential(\n",
        "            nn.Linear(2256512, 256, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, n_output, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.firstConv(x)\n",
        "        out = self.depthwiseConv(out)\n",
        "        features = self.separableConv(out)\n",
        "        # print('the shape of features before the classifier is ', features.shape)\n",
        "        out = self.classify(features)\n",
        "        return out, features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred, features = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        correct = 0\n",
        "        loss += loss_fn(pred, y).item()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 1 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            correct /= size\n",
        "            print(f\"Train Error: \\n Accuracy: {(100*correct):>0.1f}%, loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            pred, features = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "def main():\n",
        "    model = EEGNet_ReLU(3)\n",
        "    model.to(DEVICE)\n",
        "    summary(model,(1, 62, 37001))\n",
        "\n",
        "    train_data = SeedDataset('drive/MyDrive/EEGNet/processed_seed_dataset/subject1.npy')\n",
        "    test_data = SeedDataset('drive/MyDrive/EEGNet/processed_seed_dataset/subject2.npy')\n",
        "\n",
        "    train_dataloader = DataLoader(train_data, batch_size=5, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=5, shuffle=True)\n",
        "\n",
        "    # train_features, train_labels = next(iter(train_dataloader))\n",
        "    # print(f\"Feature batch shape: {train_features.size()}\")\n",
        "    # print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "\n",
        "    # The CrossEntropy loss function in PyTorch expects the target values to be in the range\n",
        "    # [0, C-1] where C is the number of classes.\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=1, momentum=0.5, weight_decay=5e-4)\n",
        "\n",
        "    epochs = 100\n",
        "    for epoch in tqdm(range(1, epochs + 1)):\n",
        "        print(f\"Epoch {epoch}\\n-------------------------------\")\n",
        "        train(train_dataloader, model, loss_fn, optimizer)\n",
        "        test(test_dataloader, model, loss_fn)\n",
        "    print(\"Done!\")"
      ],
      "metadata": {
        "id": "D9ZAVyjVLdQt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "AHVqnjXwPxg9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ZPSlPomPyUn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}